
<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
		<title>Sight and Sound - CVPR 2019</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />

    <meta property='og:title' content="Sight and Sound - CVPR 2019"/>
    <!-- <meta property='og:image' content='http://sightsound.org/dog-sight-sound11.png /> -->
    <meta property='og:image' content=sightsound-speakers-2019-v3.png /> 
    <style>
     img.rounded-img-dark {
		 border: 1px solid #cccccc;
		 border-radius: 10px ;
		 -moz-border-radius: 10px ;
		 -webkit-border-radius: 10px ;
  	}
    </style>
    
	</head>
	<body>

    <!-- <img src = "dog-sight-sound11.png" style = "display: none" width = 0> -->
    <img src = "sightsound-speakers-2019-v3.png" style = "display: none" width = 0>

		<!-- Wrapper -->
			<div id = "wrapper">

				<!-- Header-->
				<!-- <header id="header" class = "alt" style = "min-width: 100%">-->
				<header id="header" class = "alt" style = "width: 100%">
       		<!-- <span class="logo"><img height = 100px src="images/eye_ear1.jpg" alt="" /></span> -->
					<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
					<!-- <h1 style = "text-shadow: 1px 1px #000000">Sight and Sound</h1>
            -->
            <style type="text/css">
              .bgimg {
              /*background-image: url('images/cropped/dog_ear1.png');*/
              /*background-image: url('images/cropped/dog_ear4.png');*/
              /*background-image: url('images/cropped/dog_ear6.jpg');*/
              background-image: url('images/dog_sight_sound.jpg');
              background-position:center;
              /*background-align:top;*/
              }
            </style>
            <!-- <div class = "bgimg" style = "height: 475px">-->
            <div class = "bgimg" style = "height: 250px; width: 100%">
              <div style = "height: 20px; width: 100%"></div>
              <h1 style = "text-shadow: 0.1px 0.1px #000000">Sight and Sound</h1>
              <h2 style = "text-shadow: 0.1px 0.1px #000000">CVPR 2019 — Monday June 17, 9am - 5pm in Grand Ballroom B</font></h2>

              <!-- <h2 style = "text-shadow: 0.1px 0.1px #000000">CVPR 2018 workshop — June 22 in Salt Lake City</h2> -->
                 <!-- <h2 >CVPR 2018 workshop — June 22 in Salt Lake City</h2>-->
                 <!-- <h2 >CVPR 2018 — June 22, 1:30pm - 5:30pm in room 251C</h2> -->
            </div>
              <!-- cat sketch -->
            <!-- <style type="text/css"> -->
            <!--   .bgimg { -->
            <!--    background-image: url('images/cropped/cat-sketch1.jpg'); -->
            <!--    background-position:center -->
            <!--   } -->
            <!-- </style> -->
            <!--  <div class = "bgimg" style = "height: 290px"> -->
            <!--   <h1 style = "text-shadow: 0.25px 0.25px #000000"><font color = #333333> Sight and Sound</font></h1> -->
            <!--   <h2 style = "text-shadow: 0.25px 0.25px #000000"><font color = #333333>CVPR 2018 workshop</font></h2> -->

            <!-- </div> -->
            <!-- <img height = 100 src = "images/dog_ear1.jpg"> -->
            <!-- <img height = 100 src = "images/cropped/elephant_ear2_small.png"> -->
            <!-- <img height = 100 src = "images/cat_ear1.jpg">  -->
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Introduction</a></li>
							<li><a href="#schedule">Schedule</a></li>
							<!-- <li><a href="#call">Call for papers</a></li> -->
              <li><a href="#papers">Papers</a></li>
							<li><a href="#info">More info</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">


            <section id="schedule" class="main special">
							<header class="major">
								<!-- <h2>Tentative schedule</h2> -->
                <h2>Schedule</h2>
							</header>
							<!-- <p>This is a full-day workshop that will take place on -->
							  <!-- 	Monday, June 17 in Grand Ballroom B.<br> -->
                <!-- <p><font size = "+2">This workshop takes place on Monday, June 17 in <b>Grand Ballroom B</b>.</font><br> -->
              <p><div style = "width: 100%; text-align: left"><font size = "+2">This workshop is on Monday
              in <b>Grand Ballroom B</b>. <br>Directions: enter the
              conference center, go up the escalator to reach the
              2nd floor, and turn right.</font></div><br>
                <center>
                  <div class="table-wrapper" style ="width:100%">
                    <font size = "+1">
											<table class="alt">
												<tbody>
                          <col width="5%">
                          <col width="6%">
                          <col width="16%">
                          <col width="25%">
													<tr>
                            <td>9:00 - 9:05</td>
														<td>Welcome</td>
														<td></td>
														<td></td>
													</tr>
                          <tr>
                            <td>Paper session 1</td>
                            <td>9:05 - 9:15</td>
														<td>Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</td>
                            <td>Audio-Visual Event Localization in the Wild</td>
													</tr>
                          <tr>
                            <td></td>
                            <td>9:15 - 9:25</td>
                            <td>Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman</td>
                            <td>My lips are concealed: Audio-visual speech enhancement through obstructions</td>
                          <tr>
                            <td>9:25 - 9:55</td>
														<td>Invited Talk</td>
														<td><b><a href = "https://www.cs.utexas.edu/~rhgao">Ruohan Gao (UT Austin)</a></b><br><div style = "height:20px"></div>Learning to See and Hear with Unlabeled Video</td>
														<td style = "vertical-align: middle"><img class = "rounded-img-dark" src = "photos/ruohan.jpg" height = "120px"></td>
													</tr>
                          <tr>
                            <td>9:55 - 10:15</td>
														<td>Poster spotlights</td>
														<td>The authors of each poster will have the chance to present a 2-minute lighting talk.</td>
														<td></td>
													</tr>
                          <tr>
                            <td>10:15 - 11:00</td>
														<td>Poster presentations & coffee break</td>
                            <td><b>Posters will be in the same room as
                            the rest of the workshop (Grand Ballroom
                            B).</b> <strike>Poster numbers: 168 - 177</strike></td>
														<!-- <td>Poster numbers: 168 - 177</td> -->
														<td></td></tr>
                          <tr>
                            <td>11:00 - 11:30</td>
                            <td>Invited talk</td>
                            <td><b><a href = "https://www.cs.dartmouth.edu/~lorenzo/home.html">Lorenzo Torresani (Dartmouth/Facebook)</a></b><div style = "height:20px"></div>Audio-Visual Learning for Reduced Supervision and Improved Efficiency</td>
														<td style = "vertical-align: middle"><img class = "rounded-img-dark" src = "photos/lorenzo.jpg" height = "120px"></td>
													</tr>
                            <td>11:30 - 12:00</td>
														<td>Invited talk</td>
														<td><b><a href = "http://people.csail.mit.edu/talidekel/">Tali Dekel (Google)</a></b><div style = "height:20px"></div>From face to speech, and back to face</td>
														<td style = "vertical-align: middle">&nbsp;<img class = "rounded-img-dark" src = "photos/dekel.jpg" height = "120px"></td>
													</tr>
                          <tr>
                            <td>12:00 - 1:45</td>
														<td>Lunch</td>
														<td></td>
														<td></td>
													</tr>
                          <tr>
                            <td>1:45 - 2:15</td>
														<td>Invited talk</td>
														<td><b><a href = "http://web.mit.edu/torralba/www">Antonio Torralba (MIT)</a></b></td>
														<td style = "vertical-align: middle"><img class = "rounded-img-dark" src = "photos/torralba.jpg" height = "120px"></td>
													</tr>
                          <tr>
                            <td bgcolor = "#f1f1f1">2:15 - 3:00</td>
														<td bgcolor = "#f1f1f1">Keynote talk</td>
														<td bgcolor = "#f1f1f1"><b><a href = "http://web.mit.edu/jhm/www/">Josh McDermott (MIT)</a></b><br><div style = "height:20px"></div>Old and New Problems in Auditory Scene Analysis</td>
														<td bgcolor = "#f1f1f1" style = "vertical-align: middle"><img class = "rounded-img-dark" src = "photos/josh.jpg" height = "120px"></td>
													</tr>
                          <tr>
                            <td>3:00 - 3:30</td>
														<td>Invited talk</td>
														<td><b><a href = "https://people.eecs.berkeley.edu/~malik/">Jitendra Malik (UC Berkeley)</a></b>
                                <br><div style = "height:20px"></div>Learning Individual Styles of Conversational Gesture</td>
														<td style = "vertical-align: middle"><img class = "rounded-img-dark" src = "photos/jitendra.jpg" height = "140px"></td>
													</tr>
                          <tr>
                            <td>3:30 - 4:00</td>
														<td>Coffee break</td>
														<td></td>
														<td></td>
													</tr>
                          <tr>
                            <td>4:00 - 4:30</td>
														<td>Invited talk</td>
														<td><b><a href = "https://avdnoord.github.io/homepage/">Aäron van den Oord (DeepMind)</a></b</td>
														<td style = "vertical-align: middle"><img class = "rounded-img-dark" src = "photos/aaron.jpg" height = "120px"></td>
													</tr>
                          <tr>
                            <td>Paper session 2</td>
                            <td>4:30 - 4:40</td>
														<td>Joon Son Chung</td>
                            <td>Audio-visual speaker diarisation in the wild</td>
                          </tr>
                        </tbody>
                      </table>
                    </font>
                  </div>
                </center>
            </section>

            
						<!-- Introduction -->
						<section id="intro" class="main">
              <center>
                <header class="major">
								<!-- <h2>Tentative schedule</h2> -->
                <h2>Summary</h2>
							</header>
								<div class="spotlight" style = "width: 90%; text-align: left">
									<div class="content">
                    <br>
                      
                    <p>In recent years, there have been many advances
                      in learning from visual and auditory data.
                      While traditionally these modalities have been
                      studied in isolation, researchers have
                      increasingly been creating algorithms that learn
                      from both modalities.  This has produced many
                      exciting developments in automatic lip-reading,
                      multi-modal representation learning, and
                      audio-visual action recognition.
                      
                    <p>Since pretty much every internet video has an
                      audio track, the prospect of learning from
                      paired audio-visual data — either with new forms
                      of unsupervised learning, or by simply
                      incorporating sound data into existing vision
                      algorithms — is intuitively appealing, and this
                      workshop will cover recent advances in this
                      direction.  But it will also touch on
                      higher-level questions, such as what information
                      sound conveys that vision doesn't, the merits of
                      sound versus other "supplemental" modalities
                      such as text and depth, and the relationship
                      between visual motion and sound. We'll also
                      discuss how these techniques are being used to
                      create new audio-visual applications, such as in
                      the fields of speech processing and video
                      editing.
                      
                    <p><b><a href = "2018">Please click here to see last year's workshop (at CVPR 2018).</a></b> 
									    <!-- </div> -->
                  <!-- <div> -->
                    <!--   <\!-- <span class="image"><img src="images/eye_ear2.jpg" alt="" /></span>-\-> -->
                      <!--   <span><img src="images/eye_ear2.jpg" height = 140px class = "rounded-img-dark" alt="" /></span> -->
                      <!-- </div> -->
								  <!-- </div> -->

              </center>
						</section>

						<!-- <section id="call" class="main special"> -->
						<!-- 		<header class="major"> -->
						<!-- 			<h2>Call for papers</h2> -->
						<!-- 		</header> -->
            <!--     <center> -->
            <!--     <div class="spotlight"  style = "width: 90%; text-align: left"> -->
            <!--       <div class = "content"> -->
            <!--         <\!-- We'll be taking paper submission! Please check -\-> -->
            <!--         <\!-- back in the spring for more details. -\-> -->
                    
            <!--         We're inviting submissions! If you're interested -->
            <!--         in potentially presenting a poster or giving a -->
            <!--         talk, please submit a short paper (or extended -->
            <!--         abstract) to <a href = -->
            <!--         "https://cmt3.research.microsoft.com/WSS2019">CMT</a> -->
            <!--         by <b>May 1st at 11:59 PST</b>, using this -->
            <!--         LaTeX <a href = -->
            <!--         "latex/wss2019.zip">template</a>. We will notify -->
            <!--         authors about paper decisions by May <strike>10th</strike>&nbsp;13th. We -->
            <!--         encourage submissions for work that has already -->
            <!--         been accepted in other venues, as well as new, -->
            <!--         work-in-progress submissions. The paper must be at -->
            <!--         most 4 pages, including references (a 1- or 2-page -->
            <!--         extended abstract is also fine). Accepted papers -->
            <!--         will appear on this site, and on the <a href = -->
            <!--         "http://openaccess.thecvf.com/menu.py"> CVF -->
            <!--         website</a> (but not the IEEE or CVPR -->
            <!--         proceedings). Since the papers in this workshop -->
            <!--         are at most 4 pages long, they can also be -->
            <!--         submitted to next year's CVPR. <br> <div style = -->
            <!--         "height: 10px"></div>We are looking for work that -->
            <!--         involves vision and sound. For example, the -->
            <!--         following topics would be in scope: -->
            <!--         <div style = "padding-left: 2%"> -->
            <!--         <table> -->
            <!--           <tr> -->
            <!--             <td><div> -->
            <!--               <ul> -->
            <!--                 <li>Lip-reading -->
            <!--                 <li>Intuitive physics with sound -->
            <!--                 <li>Audio-visual scene understanding -->
            <!--                 <li>Sound-from-vision and vision-from-sound -->
            <!--                 <li>Audio-visual self-supervised learning -->
            <!--                 <li>Semi-supervised learning -->
            <!--               </ul></div> -->
            <!--             <td> -->
            <!--               <ul> -->
            <!--                 <li>Video-to-music alignment -->
            <!--                 <li>Video editing and movie trailer generation -->
            <!--                 <li>Material recognition -->
            <!--                 <li>Vision-inspired audio convolutional networks -->
            <!--                 <li>Sound localization -->
            <!--                 <li>Audio-visual speech processing -->
            <!--               </ul> -->
            <!--             </td> -->
            <!--         </table> -->
            <!--         </div> -->
            <!--       </div> -->
            <!--     </div> -->
            <!--     </center> -->
						<!-- 	</section> -->
              <section id = "papers" class = "main special">
                <header class="major">
									<h2>Accepted short papers</h2>
								</header>
                <!-- <ul style = "list-style: none"> -->
                  <center>
                    <div class="table-wrapper" style ="width:100%">
                      <font size = "+0.5">
											  <table class="alt">
												  <tbody>
                            <col width="40%">
                            <col width="60%">
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Lele_Chen_Sound_to_Visual_Hierarchical_Cross-Modal_Talking_Face_Generation_CVPRW_2019_paper.pdf">Sound to Visual: Hierarchical Cross-Modal Talking Face Generation</a></td><td>Lele Chen, Haitian Zheng, Ross Maddox, Zhiyao Duan, Chenliang Xu</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Event_Localization_in_the_Wild_CVPRW_2019_paper.pdf">Audio-Visual Event Localization in the Wild</a></td><td>Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, Chenliang Xu</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yapeng_Tian_Audio-Visual_Interpretable_and_Controllable_Video_Captioning_CVPRW_2019_paper.pdf">Audio-Visual Interpretable and Controllable Video Captioning</a></td><td>Yapeng Tian, Chenxiao Guan, Goodman Justin, Marc Moore, Chenliang Xu</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Inkyu_An_Reflection_and_Diffraction-Aware_Sound_Source_Localization_CVPRW_2019_paper.pdf">Reflection and Diffraction-Aware Sound Source Localization</a></td><td>Inkyu An, Jung-Woo Choi, Dinesh Manocha, Sung-Eui Yoon</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Yukitaka_Tsuchiya_Generating_Video_from_Single_Image_and_Sound_CVPRW_2019_paper.pdf">Generating Video from Single Image and Sound</a></td><td>	Yukitaka Tsuchiya, Takahiro Itazuri, Ryota Natsume, Shintaro Yamamoto, Takuya Kato, Shigeo Morishima</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Amanda_Cardoso_Duarte_WAV2PIX_Speech-conditioned_Face_Generation_using_Generative_Adversarial_Networks_CVPRW_2019_paper.pdf">WAV2PIX: Speech-conditioned Face Generation using Generative Adversarial Networks</a></td><td>	Amanda Cardoso Duarte, Francisco Roldan, Miquel Tubau, Janna Escur, Santiago Pascual, Amaia Salvador, Eva Mohedano, Kevin McGuinness, Jordi Torres, Xavier Giro-i-Nieto</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Naji_Khosravan_On_Attention_Modules_for_Audio-Visual_Synchronization_CVPRW_2019_paper.pdf">On Attention Modules for Audio-Visual Synchronization</a></td><td>	Naji Khosravan, Shervin Ardeshir, Rohit Puri</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Angie_W_Boggust_Grounding_Spoken_Words_in_Unlabeled_Video_CVPRW_2019_paper.pdf">Grounding Spoken Words in Unlabeled Video</a></td><td>	Angie W Boggust, Kartik Audhkhasi, Dhiraj Joshi, David Harwath, Samuel Thomas, Rogerio Feris, Danny Gutfreund, Yang Zhang, Antonio Torralba, Michael Picheny, James Glass</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/German_Parisi_A_Neurorobotic_Experiment_for_Crossmodal_Conflict_Resolution_CVPRW_2019_paper.pdf">A Neurorobotic Experiment for Crossmodal Conflict Resolution</a></td><td>	German Parisi, Pablo Barros, Di Fu, Sven Magg, Haiyan Wu, Xun Liu, Stefan Wermter</td></tr>
                            <tr><td><a href = "http://openaccess.thecvf.com/content_CVPRW_2019/papers/Sight%20and%20Sound/Konstantinos_Vougioukas_End-to-End_Speech-Driven_Realistic_Facial_Animation_with_Temporal_GANs_CVPRW_2019_paper.pdf">End-to-End Speech-Driven Realistic Facial Animation with Temporal GANs</a></td><td>	Konstantinos Vougioukas, Stavros Petridis, Maja Pantic</td></tr>
                          </tbody>
                        </table>
                      </font>
                      <div style = "width: 90%; text-align: left">
                        Posters should follow the <a href = "http://cvpr2019.thecvf.com/">CVPR 2019 format</a>. Oral presentations should be no more than 10 minutes long (including questions). Papers are available via the <a href = "http://openaccess.thecvf.com/CVPR2019_workshops/CVPR2019_Sight_and_Sound.py">Computer Vision Foundation</a>.
                      </div>                      
              </section>
							<section id="info" class="main special">
								<header class="major">
									<h2>Organizers</h2>
                </header>
                  <!-- <ul style = "list-style: none">
                  <!--   <li><a href = "http://andrewowens.com">Andrew Owens (UC Berkeley)</a> -->
                  <!--   <li><a href = "https://jiajunwu.com">Jiajun Wu (MIT)</a> -->
                  <!--   <li><a href = "https://billf.mit.edu">William Freeman (MIT/Google)</a> -->
                  <!--   <li><a href = "https://www.robots.ox.ac.uk/~az">Andrew Zisserman (Oxford)</a> -->
                  <!--   <li><a href = "http://people.inf.ethz.ch/jebazin/CML">Jean-Charles Bazin (KAIST)</a> -->
                  <!--   <li><a href = "https://www.microsoft.com/en-us/research/people/zhang">Zhengyou Zhang (Microsoft Research)</a> -->
                  <!--   <li><a href = "http://web.mit.edu/torralba/www">Antonio Torralba (MIT)</a> -->
                  <!--   <li><a href = "https://people.eecs.berkeley.edu/~efros">Alexei Efros (UC Berkeley)</a> -->
                  <!-- </ul> -->
                  <div class="table-wrapper" style ="width:100%">
                    <font size = "+0.5">
											<table cellpadding = "0" cellspacing = "0">
                        <!-- <tr><td><img class = "rounded-img-dark" height = "125px" src = "photos/owens.jpg"></td></tr> -->
                        <!-- <tr><td>Andrew Owens</td></tr> -->
                        <tr>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/owens.jpg"><br><a href = "http://andrewowens.com">Andrew Owens<br>UC Berkeley</a></td>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/wu.jpg"><br><a href = "https://jiajunwu.com">Jiajun Wu<br>MIT</a></td>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/freeman.jpg"><br><a href = "https://billf.mit.edu">William Freeman<br>MIT/Google</a></td>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/zisserman.jpg"><br><a href = "https://www.robots.ox.ac.uk/~az">Andrew Zisserman<br>Oxford</td>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/bazin.jpg"><br><a href = "http://people.inf.ethz.ch/jebazin/CML">Jean-Charles Bazin<br>KAIST</td>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/zhang.jpg"><br><a href = "https://www.microsoft.com/en-us/research/people/zhang">Zhengyou Zhang<br>Tencent</a>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/torralba.jpg"><br><a href = "http://web.mit.edu/torralba/www">Antonio Torralba<br>MIT</a>
                          <td><img class = "rounded-img-dark" height = "125px" src = "photos/grauman.jpg"><br><a href = "http://www.cs.utexas.edu/users/grauman">Kristen Grauman<br>UT Austin / Facebook</a>
                        </tr>
                      </table>
                    </font>
                  </div>
                            
              <br>
							</section>
					</div>

                 
   				<!-- Footer -->
					<footer id="footer">
						<!-- <p class="copyright">&copy; Andrew Owens.  -->
            <p class = "copyright"><font size = "-1"> Website design adapted
						from <a href = "http://bayesiandeeplearning.org">Yarin
						Gal</a> and based on a template
						from <a href="https://html5up.net">HTML5 UP</a>.  Dog eye/ear photo from <a href = "https://www.gettyimages.com/license/82782291" rel="noreferrer">Compassionate Eye Foundation/Getty Images Staff Photographer</a>.</font> </p>
					</footer>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]>
          <script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<!-- <script src="assets/js/main.js"></script> -->

	</body>
</html>
